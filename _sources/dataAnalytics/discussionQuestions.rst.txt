.. highlight:: r

*************************************************
Disucssion Questions:
*************************************************
by Joshua Peterson (created in Sphinx)



Week 1
=================

Analytics requires the application of statistics, computer programming, and advanced analytical methods.  The combination of all three allows for the discovery, interpretation, and communication of valuable information and insights extracted from data.  Based on Wikipedia [1] there are multiple types of analytics some of the more common ones today include:
    * Risk analytics (predictive models that give risk score to individual customers for applications such as loans
    * Digital Analytics (convert digital information into useful information for reporting, prediction, ect.
    * Security Analytics (gather and analysis information about different security events and breaches)

Also based on the Wikipedia article [1] others types of analytics include:
    * Architectural analytics
    * Behavioral analytics
    * Big data analytics
    * Business Analytics
    * Cloud analytics
    * Continuous analytics
    * Cultural Analytics
    * Customer Analytics
    * Embedded analytics
    * Learning Analytics
    * News Analytics
    * Online video analytics
    * Predictive analytics
    * Predictive engineering analytics
    * Prescriptive analytics
    * Semantic analytics
    * Social analytics
    * Software analytics
    * Speech analytics
    * User behavior analytics
    * Visual Analytics
    * Web analytics
    * Win-loss analytics

Analytics looks at the entire process starting with determining the question, collecting the data, cleaning up the data, analyzing the information, interpolate the results, and come up with recommendations and conclusions.  However, the analysis is just one step along the way which starts with cleaned data and ends with interpreting the results to come up with conclusions.



Reference

[1] "Analytics." Wikipedia. Wikimedia Foundation, 04 Mar. 2017. Web. 14 Mar. 2017.



Week 2
=================

*Please post your answers (reply to this thread) to the questions below and respond to at least one other persons post.

 Initial post due - March 22nd, reply - March 26th, midnight your time.

Discuss research/experimenter bias. How does ANOVA help overcome this?*


There are multiple types of bias including selection bias, voluntary response bias, convenience sample bias, recall bias, latent bias, etc.  Many of the bias can be reduced through well-designed experiments and philosophies such as using placebos, doing double blind studies, and performing crossover studies.  Different statistical methods can also be used to flag research/experimenter bias such as the statistical method ANOVA (analysis of variance).  ANOVA describes how much variance is between different groups versus how much is within each group.  If the research/experimenter bias is systemic, then there may be significant variations within the groups making it much harder to reject the null hypothesis. bias.

For experimental design, there is three main type of design approaches: completely randomized design,  randomized block design, and factorial design.  The completely randomized design is used to study one primary factor and does not take into consideration other factors.  This method requires the least amount of experiments which works well if there truly is no other factor influencing the result.

The randomized block design is used to remove unimportant influences into the experiment.  This method requires first group subjects into blocks based on characteristics such as age, gender, location, etc. and then the subjects are then randomized within each block.  The randomized block design requires more experiments than completely randomized design, but it does eliminate effects associated with different unimportant influences.

The factorial design is used to look at the effects associated with multiple influences.  In this method, all influences are considered significant and are analyzed.  This method requires the most amount of experiments but does lead to the maximum amount of information.



Week 3
=======

Most inferential statistics is based on the assumption that the variable being measured are normally distributed though it is not required for all inferential statistical analysis.   Perfect normally distributed can be determined if the mean is equal to the median as well as the mode.   The reason it is desirous to have a normal distribution that based on the central limit theorem that states if the variables are independent and random then the distribution of the variables will approximate normal as N increases regardless the shape of the population distribution.

There should always be some suspicion of the data if it does not approach a normal distribution.  The problems with the data could include outliers, insufficient data, or inappropriately graphed data.

It is possible to determine if something is normal by conducting the test of the "Assumption of Normality" which can include the Kolmogorov-Smirnov or Shapiro-Wilk test.  These test test the shape against a variety of popular shapes.  Based on these test it can be concluded that if the p-value is under 0.05, then there is statistically significant evidence that the sample is not normal.

If the data is determined to not have a normal distribution, then methods can be used to transform the data into a normal distribution so the analysis can be performed assuming normalized and then transform the results back after completion.  Also, there is a section of statistics called nonparametric statistics that can be used because it makes no assumptions about the data distribution.

Week 4
==========



The main graphs I have used during my work at ORNL as a reactor physics analyst include text tables, highlight tables, symbol map, fill map, pie charts, bar charts, lines plots, area plots, scatter plots, histogram, and box-and-whisker plots.  I use text tables for sharing detailed data with others who may be interested in using it in the future.

I like using highlight tables to show how the data in the table differs especially when the user may be interested in the local maximums or minimums.  I like to use symbol map and fill map when I am displaying information on the physical space with different data points such as the location of spent fuel in the USA.

I don't like pie charts at all, but I will use them once in a while to show the relationship of different items compared to the whole.  Bar charts are useful for showing discrete trends in the data and for grouping the data over time.  The line chart is also useful for displaying continuous variable over time such as helping the user to see how things increase and decrease over time.

I typically used scatter plots to show the relationship of multidimensional data and also look for outliers within this data. Scatter plot is one of my favourite scenarios to use.  A newer scheme that I have been using lately is a box-and-whisker plot.  It is excellent at summarizing a significant amount of info into one chart to determine reasonable ranges of the data.

Some of the plots I have generated in the past can be seen on the website  https://curie.ornl.gov/map.  On this website, I use pie, line, bar, symbol map, and fill the map.  It was programmed by one of the GIS folks where I work at in D3.


The article on the 14 best data visualization tools can be found on the website located at http://thenextweb.com/dd/2015/04/21/the-14-best-data-visualization-tools/.  The tools that I have used named on this site include D3.js and Tableau.  I have been using Tableau for about a year now and find it to be the best interactive visualization tool out there.  Tableau allows for the user to rapidly develope graphs and then uses those figures to dive deeper into questions about the data.  The biggest problem with Tableau is the cost.  Though Tableau makes fantastic graphs that can be displayed online, hosting it on my work website could cost 10K-100K a year which is above my budget.

D3 also has the capability of producing very stimulating graphs that are interactive.  D3 is an open source project, and as such, it is free to post online.  However, D3 does take some work and customization to produce usable results.  I haven't tried this, but I think a good approach for data visualization would be to use the desktop version of Tableau to "wireframe" what the dashboard should look like and then use D3 to try and reproduce the look and feel of Tableau.

Aother tools I have used for data visualization includes ggplot in R, Pyplot,  RapidMiner, and Orange.

Week 5
=========

Discuss some practical applications of text mining and/or discuss the benefits text mining can have for a company...any downfalls?


Text mining can be used for multiple applications across a vast range of industry.  One the website  http://www.expertsystem.com/10-text-mining-examples/ the examples include:  risk management, knowledge management, cybercrime prevention, customer care service, fraud detection through claims invenstigation, contextual advertising, buisness intelligence, content enrichment, spam filtering, social media data analysis.  It seems the main power behind text mining is the ability to access large amounts of data to allow analysist to quickly answer the question.  Google has continued to improve their ability in text mining the web to the point that when someone wants to know an answer to a question they can just "google" it.  With the adaption of the populatiryt of social media, a branch of text mining called sentemental analysis has become a powerful tool for gaining up to date knowledge about peoples belief and feels about current issues.  Soon I imagine sentamental analysis on social medial sites such as twitter could be used for playing the stock market, testing effectiveness of advertising campains, and better predicting election results.

There are many downfalls of text mining including the quality of the data especailly with the relatization behind the power of "fake news".  In addition, tools such as sentimental analysis currently do not allways capture the correct meaning behind peoples writitng.  For example I was extracting data from twitter the other day and I noticed a lot of the comments came up negative.  After further investigation I realized I was getting informaiton during a Raiders football game and fans were twitting about it and the sentamental analysis tool thought the word "Raider" was negative and assigning a fales sentement to it.

Week 7:
==========

*Give some examples of sequential decision making problems.*

In sequential decision-making a decision is made based on the observed outcome the results are evaluated then a second decion is made based on the potential outcomes, and then a third decsion is made based on the potentail outcomes so on.  The decision-maker/machines should look at future decisions before making the one now.  Thuse they know what alternatives to select when decisions are needed.  Decision trees are useful for modeling these sectionas. An example of sequential decision-making is robot navigation and playing games such as chess.  In both the learning takes place in an ever changing environment where the machines need to perform based on the current state.  With every chose that is made is an associated reward.  The machine learns what action to take to optimzie the total reward.  In the book "Algorithms for Reinforcment Learning" by C. Szepesvari other example include inventory control (what inventory should be ordered every day based on the current state of the system and what is expected to be purchased in the future), optimizng transportation systems, opimzing schedules and productions, optimal portfoli management, options pricing

*Perform an Internet search and see if you can find mention of algorithms for reinforcement learning.:*

In the book "Algorithms for Reinforcment Learning" by C. Szepesvari examples of reinforcment learning algorthims include Markov Decision Processes, Value functions, Tabulra, Every-visit Monte-Carlo, temporal difference with function approximation, gradient temporal difference learning, least-square methods, q-learning in finitie Markov Descions Process, Q-learning with function approximation.  In another book "Scala: Applied Machine Learning by P. Bugnion et. al". he has a tree decribing algorithsm for reinfocment learning.   Under Markov is modeled based, i.e. iterative value and iterative policy and tempoeral differences, i.e.   Q-learning and SARSA.  Uner Evolution there is rules such as learning cliassifers and XCS and optimzer such as stochastic gradient and genetic algorithm.


Week 8 Operational Research
==============================


The focus of operational research is to investigate operations where operations are defined as a set of steps required to achieve the desired outcome.  In other words, operational research deals with optimization and decision making of a wide range of business projects.  The goal of the operational research is produced better decisions, improve coordination, have better control, and improve the overall system.  This is accomplished by optimizing the desired outcome by adjusting the examining all of the different options all while taking into consideration the constraints.

On the other hand data scientist look deep into data needs to find the desired results which are similar to operations research if the result you want is optimization of a system.  Thus operational research and data science are related because many of the algorithms in OR can also be applied to the world of data.  However, when the goal is not the optimization of a system data science will require a larger depth and breadth of tools needed to dive down deep into the messy data to provide the needed inside.  A good analogy is taken from the website https://www.dezyre.com/article/data-science-compared-with-different-analytics-disciplines/175:  "If operations research is the metal detector that guides to the right area of business, then data science is the spade to dig into the data and extract value.
