
==============
Week6 Spark
==============
by Josh Peterson



Installing CentOS and VMFusion
===============================

* Installed CentOS7 64-bit (minimal install) with networking on, date and time set, and kdump disability

* I then set a root password and also created a user name.  

 

* Because I am using just the minimal install there was no user interface so I decided to interact with CentOS7 through ssh from my mac.  I like iterm better on mac then its original terminal so I used that for ssh into the machine.  The first step was to find my ip address using command::

    ip addr
    
This is used to determine my ip address was *172.16.84.129*::

    ssh 172.16.84.129

* Once I was able to ssh to the CentOS7 vitural machine I installed the OS updates doing::

    su
    yum update
    yum install wget 



Installing Java
===============

I installed java by doing the following::
    
    cd /opt
    wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz     
    tar xzf jdk-8u111-linux-x64.tar.gz   
    chown -R root:root jdk1.8.0_111/   
    
I then used alternative to install java doing the following: (Alternative allows for softlinks to the desired location of the executable so as different version of java are released they can be installed and then alternatives used to link to a new one without having to uninstall the old.  This allows for the user to also test the new release and easily go back to the previous one if problems occur):: 
 
    cd /opt/jdk1.8.0.111
    alternatives --install /usr/bin/java java /opt/jdk1.8.0_111/bin/java 2    
    alternatives --config java
    alternatives --install /usr/bin/jar jar /opt/jdk1.8.0_111/bin/jar 2
    alternatives --install /usr/bin/javac javac /opt/jdk1.8.0_111/bin/javac 2
    alternatives --set jar /opt/jdk1.8.0_111/bin/jar
    alternatives --set javac /opt/jdk1.8.0_111/bin/javac

The next step is to create the Hadoop user with commands::
   

    useradd hadoop
    passwd hadoop

To allow me the ability for password-less login in using ssh-keygen I do the following::

    su - hadoop
    ssh-keygen -t rsa
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    chmod 0600 ~/.ssh/authorized_keys

Once I have established the new user I need to add some environment variables before installing Hadoop.  These are created by modifying the .bashrc file for Hadoop.  This is accomplished by adding the following to the .bashrc::

    export JAVA_HOME=/opt/jdk1.8.0_111
    export JRE_HOME=/opt/jdk1.8.0_111/jre
    export PATH=$PATH:$JAVA_HOME/bin

I then source the .bashrc to restart it so I do not need to log out and log back in to get access to it
To edit the .bashrc I like to use vi instead of nano.  I also use ultraedit for mac to edit longer files through ssh instead 

Install Scala
=============

Scala is needed for installing spark.  The commands for installing scala is::

    cd ~
    wget http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.rpm
    sudo yum install scala-2.11.8.rpm   
 
.. note::

    These directions for installing scala come from website https://www.vultr.com/docs/how-to-install-scala-on-centos-7
 
When installing scala the following output is seen::

    [root@localhost hadoop]#     sudo yum install scala-2.11.8.rpm
    Loaded plugins: fastestmirror
    Examining scala-2.11.8.rpm: scala-2.11.8-0.noarch
    Marking scala-2.11.8.rpm to be installed
    Resolving Dependencies
    --> Running transaction check
    ---> Package scala.noarch 0:2.11.8-0 will be installed
    --> Finished Dependency Resolution
    
    Dependencies Resolved
    
    =====================================================================================================
     Package             Arch                 Version                  Repository                   Size
    =====================================================================================================
    Installing:
     scala               noarch               2.11.8-0                 /scala-2.11.8               471 M
    
    Transaction Summary
    =====================================================================================================
    Install  1 Package
    
    Total size: 471 M
    Installed size: 471 M
    Is this ok [y/d/N]: y
    Downloading packages:
    Running transaction check
    Running transaction test
    Transaction test succeeded
    Running transaction
      Installing : scala-2.11.8-0.noarch                                                             1/1
      Verifying  : scala-2.11.8-0.noarch                                                             1/1
    
    Installed:
      scala.noarch 0:2.11.8-0
    
    Complete!

Scala is installed in the bin directory





Installing Hadoop
=================

First I login as the Hadoop user using::

    su - hadoop
    wget http://apache.claz.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
    tar xzf hadoop-2.7.3.tar.gz
    mv hadoop-2.7.3 hadoop


I then rename the hadoop-2.7.3 folder to hadoop and then add the following variables to the .bashrc and once added restart the .bashrc::

    export HADOOP_HOME=/home/hadoop/hadoop
    export HADOOP_INSTALL=$HADOOP_HOME
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

I then run::

	source ~/.bashrc
	
I then change directories::

    cd $HADOOP_HOME/etc/hadoop

In this directory there are five main xml files that need to be customized.

* core-site.xml::

    <configuration>
    <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
    </property>
    </configuration>


* hdfs.site.xml::

    <configuration>
    <property>
    <name>dfs.replication</name>
    <value>1</value>
    </property>
    
    <property>
    <name>dfs.name.dir</name>
    
    <value>file:///home/hadoop/hadoopdata/hdfs/namenode</value>
    </property>
    
    <property>
    <name>dfs.data.dir</name>
    <value>file:///home/hadoop/hadoopdata/hdfs/datanode</value>
    </property>
    </configuration>
    
* mapred-site.xml(cp map-site.xml.tempalte to map-sit.xml)::

    <configuration>
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
    </configuration>
    
* yarn-site.xml::

    <configuration>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    </configuration>

After editing the xml then I format the namenode (namenode is the centerpiece of the HDFS file system and is also known as master.  It is a single point of failure and if it goes down all of the system goes down)::

    hdfs namenode -format
    
I then start HDFS, YARN, and HistoryServer using the command::

    start-dfs.sh
    start-yarn.sh
    mr-jobhistory-daemon.sh start historyserver

To see all of the java jobs running I type::

    jps

The following java scripts should be seen::

    32034 Jps
    31303 DataNode
    31449 SecondaryNameNode
    31594 ResourceManager
    31695 NodeManager
    31999 JobHistoryServer

I then disabled the firewall using the command::
    su
    systemctl stop firewalld

I then disabled the IPV6 by adding these lines to /etc/sysctl.conf::

    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1
    
    
Install Spark
==============

.. note:: 

    Steps for installing spark come frome the following website.  https://www.tutorialspoint.com/apache_spark/apache_spark_installation.htm

To install spark I use the following commands::

    wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz
    tar -xvf spark-2.1.0-bin-hadoop2.7.tgz
    mv spark-2.1.0-bin-hadoop2.7 spark
    
I then edit the .bashrc and add the following::

    export PATH=$PATH:/home/hadoop/spark/bin

I then do the following::

    source ~/.bashrc
    
To run spark I do the following command::

    spark-shell
    
The output is as follows::

    [hadoop@localhost bin]$ spark-shell
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    17/02/19 20:08:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    17/02/19 20:08:14 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 172.16.84.129 instead (on interface ens33)
    17/02/19 20:08:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
    17/02/19 20:08:23 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
    17/02/19 20:08:23 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
    17/02/19 20:08:24 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
    Spark context Web UI available at http://172.16.84.129:4040
    Spark context available as 'sc' (master = local[*], app id = local-1487552895739).
    Spark session available as 'spark'.
    Welcome to
          ____              __
         / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
       /___/ .__/\_,_/_/ /_/\_\   version 2.1.0
          /_/
    
    Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111)
    Type in expressions to have them evaluated.
    Type :help for more information.  



The website for spark can be found at http://172.16.84.129:4040

.. figure:: _static/sparkwebpage.png
    

Testing Spark
=============

To test spark I first create the file input.txt::

    people are not as beautiful as they look, 
    as they walk or as they talk.
    they are only as beautiful  as they love, 
    as they care as they share.
    

Then I run spark-shell.  Then I creat an Resilient Distributed Datasets (RDD) using the command::

    val inputfile =sc.textFile("input.txt")

Then I split the words by a space and then give each word 1 point and then command all of the common words to reducedByKey and add each associated pair.  All of this in just one command  :: 

    val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
    
Then applying the action and save the output ::

    counts.saveAsTextFile("output")
    
Then you can check the output and view it which shows the following unger part-0000 of the file output vi part-00000::

    (talk.,1)
    (are,2)
    (not,1)
    (people,1)
    (share.,1)
    (or,1)
    (only,1)
    (as,8)
    (,1)
    (care,1)
    (they,7)
    (beautiful,2)
    (walk,1)
    (love,,1)
    (look,,1)   



    


Useful Spark Commands
=====================

.. note::  This information was taken from website https://www.tutorialspoint.com/apache_spark/apache_spark_core_programming.htm

Transformation and Meaning::

    map(func)
    filter(func)
    flatMap(func)
    mapPartitions(func)
    mapPartitionsWithIndex(func)
    sample(withReplacement,fraction,seed)
    union(otherDatatset)
    intersection(otherDataset)
    distinct([numTasks])
    groupByKey([numTaks])
    reducedByKey(func,[numTasks])
    aggregatedByKey(zeroValue)(seqOp,combOp,[numTasks])
    sortByKey([ascending],[numTasks])
    join(otherDataset,[numTasks])
    cogroup(otherDataset,[numTasks])
    cartesian(otherDataset)
    pipe(command,[envVars])
    coalesce(numPartitions)
    rapartition(numPartitions)
    repartitionAndSortWithinPartitions(partitioner)
    
Actions::
    
    reduce(func)
    collect()
    count()
    first()
    taken(n)
    takeSample(withReplacement,num,[seed])
    takeOrdered(n,[ordering])
    saveAsTestFile(path)
    saveAsSequenceFile(path)(Java and Scala)
    saveAsObjectFile(path)(Java and Scala)
    countByKey()
    foreach(func)
    

 
    
Useful HDFS commands
=====================

Here is a list of usefull commands taken from website:  https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html::

    bin/hadoop fs <args>
    hadoop fs -appendToFile <localsrc> ... <dst>
    hadoop fs -cat [-ignoreCrc] URI [URI ...]
    hadoop fs -copyFromLocal <localsrc> URI
    hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>
    hadoop fs -find <path> ... <expression> ...
    hadoop fs -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] <args>
    hadoop fs -mkdir [-p] <paths>
    hadoop fs -tail [-f] URI