==============
Week4 HIVE
==============
by Josh Peterson


Installing CentOS and VMFusion
===============================

* Installed CentOS7 64-bit (minimal install) with networking on, date and time set, and kdump disability

* I then set a root password and also created a user name.  

 

* Because I am using just the minimal install there was no user interface so I decided to interact with CentOS7 through ssh from my mac.  I like iterm better on mac then its original terminal so I used that for ssh into the machine.  The first step was to find my ip address using command:

    ip addr
    
This is used to determine my ip address was â€œ192.168.112.135::

    ssh 192.168.112.135

* Once I was able to ssh to the CentOS7 vitural machine I installed the OS updates doing::

    su
    yum update
    yum install wget 



Installing Java
===============

I installed java by doing the following::
    
    cd /opt
    wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jdk-8u111-linux-x64.tar.gz     
    tar xzf jdk-8u111-linux-x64.tar.gz   
    chown -R root:root jdk1.8.0_111/   
    
I then used alternative to install java doing the following: (Alternative allows for softlinks to the desired location of the executable so as different version of java are released they can be installed and then alternatives used to link to a new one without having to uninstall the old.  This allows for the user to also test the new release and easily go back to the previous one if problems occur):: 
 
    cd /opt/jdk1.8.0.111
    alternatives --install /usr/bin/java java /opt/jdk1.8.0_111/bin/java 2    
    alternatives --config java
    alternatives --install /usr/bin/jar jar /opt/jdk1.8.0_111/bin/jar 2
    alternatives --install /usr/bin/javac javac /opt/jdk1.8.0_111/bin/javac 2
    alternatives --set jar /opt/jdk1.8.0_111/bin/jar
    alternatives --set javac /opt/jdk1.8.0_111/bin/javac

The next step is to create the Hadoop user with commands::
   

    useradd hadoop
    passwd hadoop

To allow me the ability for password-less login in using ssh-keygen I do the following::

    su - hadoop
    ssh-keygen -t rsa
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    chmod 0600 ~/.ssh/authorized_keys

Once I have established the new user I need to add some environment variables before installing Hadoop.  These are created by modifying the .bashrc file for Hadoop.  This is accomplished by adding the following to the .bashrc::

    export JAVA_HOME=/opt/jdk1.8.0_111
    export JRE_HOME=/opt/jdk1.8.0_111/jre
    export PATH=$PATH:$JAVA_HOME/bin

I then source the .bashrc to restart it so I do not need to log out and log back in to get access to it
To edit the .bashrc I like to use vi instead of nano.  I also use ultraedit for mac to edit longer files through ssh instead 



Installing Hadoop
=================

First I login as the Hadoop user using::

    su - hadoop
    wget http://apache.claz.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
    tar xzf hadoop-2.7.3.tar.gz
    mv hadoop-2.7.3 hadoop


I then rename the hadoop-2.7.3 folder to hadoop and then add the following variables to the .bashrc and once added restart the .bashrc::

    export HADOOP_HOME=/home/hadoop/hadoop
    export HADOOP_INSTALL=$HADOOP_HOME
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin


I then change directories::

    cd $HADOOP_HOME/etc/hadoop

In this directory there are five main xml files that need to be customized.

* core-site.xml::

    <configuration>
    <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
    </property>
    </configuration>


* hdfs.site.xml::

    <configuration>
    <property>
    <name>dfs.replication</name>
    <value>1</value>
    </property>
    
    <property>
    <name>dfs.name.dir</name>
    
    <value>file:///home/hadoop/hadoopdata/hdfs/namenode</value>
    </property>
    
    <property>
    <name>dfs.data.dir</name>
    <value>file:///home/hadoop/hadoopdata/hdfs/datanode</value>
    </property>
    </configuration>
    
* mapred-site.xml::

    <configuration>
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
    </configuration>
    
* yarn-site.xml::

    <configuration>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    </configuration>

After editing the xml then I format the namenode (namenode is the centerpiece of the HDFS file system and is also known as master.  It is a single point of failure and if it goes down all of the system goes down)::

    hdfs namenode -format
    
I then start HDFS, YARN, and HistoryServer using the command::

    start-dfs.sh
    start-yarn.sh
    mr-jobhistory-daemon.sh start historyserver

To see all of the java jobs running I type::

    jps

The following java scripts should be seen::

    32034 Jps
    31303 DataNode
    31449 SecondaryNameNode
    31594 ResourceManager
    31695 NodeManager
    31999 JobHistoryServer

I then disabled the firewall using the command::
    su
    systemctl stop firewalld

I then disabled the IPV6 by adding these lines to /etc/sysctl.conf::

    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1
    
    
    
    
Installing HIVE
================
..

    I create the user hive with the commands::
       
    
        su
        useradd hive
        passwd hive
    
        
    To allow me the ability for password-less login in using ssh-keygen I do the following::
    
        su - hive
        ssh-keygen -t rsa
        cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
        chmod 0600 ~/.ssh/authorized_keys    
    
    
    I then add the following to the .bashrc::
    
    
        export JAVA_HOME=/opt/jdk1.8.0_111
        export JRE_HOME=/opt/jdk1.8.0_111/jre
        export PATH=$PATH:$JAVA_HOME/bin
    
    
        export HADOOP_HOME=/home/hadoop/hadoop
        export HADOOP_INSTALL=$HADOOP_HOME
        export HADOOP_MAPRED_HOME=$HADOOP_HOME
        export HADOOP_COMMON_HOME=$HADOOP_HOME
        export HADOOP_HDFS_HOME=$HADOOP_HOME
        export YARN_HOME=$HADOOP_HOME
        export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

I then get the file using the command::

    wget http://mirror.metrocast.net/apache/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz
    tar -xzvf apache-hive-2.1.1-bin.tar.gz

   

I then add the following to my .bashrc::

    export PATH=/home/hadoop/apache-hive-2.1.1-bin/bin:$PATH
    
My new .bashrc file looks like the following::


    # User specific aliases and functions
    
    export JAVA_HOME=/opt/jdk1.8.0_111
    export JRE_HOME=/opt/jdk1.8.0_111/jre
    export PATH=$PATH:$JAVA_HOME/bin
    
    
    export HADOOP_HOME=/home/hadoop/hadoop
    export HADOOP_INSTALL=$HADOOP_HOME
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
    
    export HIVE_HOME=/home/hadoop/apache-hive-2.1.1-bin
    export PATH=$HIVE_HOME/bin:$PATH
    
    
 
I then run::

    source ~/.bashrc   
    
I then create folders in hdfs using the commands::

    hadoop fs -mkdir /tmp
    hadoop fs -mkdir /user
    hadoop fs -mkdir /user/hive
    hadoop fs -mkdir /user/hive/warehouse
    hadoop fs -chmod g+w /tmp
    hadoop fs -chmod g+w /user/hive/warehouse
    
Then i run hive using command::

    schematool -dbType derby -initSchema
    
    hive
    
The output from this command is::

    [hadoop@localhost ~]$     schematool -dbType derby -initSchema
    which: no hbase in (/home/hadoop/apache-hive-2.1.1-bin/bin:/home/hadoop/apache-hive-2.1.1-bin/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/jdk1.8.0_111/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin:/home/hadoop/.local/bin:/home/hadoop/bin:/opt/jdk1.8.0_111/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin:/opt/jdk1.8.0_111/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin)
    SLF4J: Class path contains multiple SLF4J bindings.
    SLF4J: Found binding in [jar:file:/home/hadoop/apache-hive-2.1.1-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/home/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
    Metastore connection URL:	 jdbc:derby:;databaseName=metastore_db;create=true
    Metastore Connection Driver :	 org.apache.derby.jdbc.EmbeddedDriver
    Metastore connection User:	 APP
    Starting metastore schema initialization to 2.1.0
    Initialization script hive-schema-2.1.0.derby.sql
    Initialization script completed
    schemaTool completed
    
    
The output from running hive is::

    [hadoop@localhost ~]$ hive
    which: no hbase in (/home/hadoop/apache-hive-2.1.1-bin/bin:/home/hadoop/apache-hive-2.1.1-bin/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/jdk1.8.0_111/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin:/home/hadoop/.local/bin:/home/hadoop/bin:/opt/jdk1.8.0_111/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin:/opt/jdk1.8.0_111/bin:/home/hadoop/hadoop/sbin:/home/hadoop/hadoop/bin)
    SLF4J: Class path contains multiple SLF4J bindings.
    SLF4J: Found binding in [jar:file:/home/hadoop/apache-hive-2.1.1-bin/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/home/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
    
    Logging initialized using configuration in jar:file:/home/hadoop/apache-hive-2.1.1-bin/lib/hive-common-2.1.1.jar!/hive-log4j2.properties Async: true
    Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
    
    
DDL Operations
=====================

I used the following commands to create HIVE tables::

    CREATE TABLE pokes (foo INT, bar STRING);
    CREATE TABLE invites (foo INT, bar STRING);
    
The output of the  command are::

    hive> CREATE TABLE pokes (foo INT, bar STRING);
    OK
    Time taken: 1.783 seconds
    hive> CREATE TABLE invites (foo INT, bar STRING)
        > ;
    OK
    Time taken: 0.144 seconds

    
I used the following commands to brows tables::

    SHOW TABLES;
    SHOW TABLES '.*s';
    DESCRIBE invites;

The output is sas follows::
    hive> SHOW TABLES;
    OK
    invites
    pokes
    Time taken: 0.199 seconds, Fetched: 2 row(s)
    hive> SHOW TABLES '.*s';
    OK
    invites
    pokes
    Time taken: 0.035 seconds, Fetched: 2 row(s)


SQL Operations
==============




The commands::

    
    wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
    
    su
    yum install unzip
    exit
    
    unzip ml-100k
    
    hadoop fs -moveFromLocal  ml-100k  /user/hive
    
The hive command is ::

    LOAD DATA  INPATH '/user/hive/ml-100k/u.data'  OVERWRITE INTO TABLE u_data;

The output is ::

    hive> LOAD DATA  INPATH '/user/hive/ml-100k/u.data'  OVERWRITE INTO TABLE u_data;
    Loading data to table default.u_data
    OK
    Time taken: 1.077 seconds
    
   
The hive command is ::
    select count(*) from u_data;

The output is ::

    hive> select count(*) from u_data;
    WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
    Query ID = hadoop_20170208214539_e5edf74e-d4c1-4557-81d3-ab42aac6d647
    Total jobs = 1
    Launching Job 1 out of 1
    Number of reduce tasks determined at compile time: 1
    In order to change the average load for a reducer (in bytes):
      set hive.exec.reducers.bytes.per.reducer=<number>
    In order to limit the maximum number of reducers:
      set hive.exec.reducers.max=<number>
    In order to set a constant number of reducers:
      set mapreduce.job.reduces=<number>
    Starting Job = job_1486604270841_0001, Tracking URL = http://localhost:8088/proxy/application_1486604270841_0001/
    Kill Command = /home/hadoop/hadoop/bin/hadoop job  -kill job_1486604270841_0001
    Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
    2017-02-08 21:46:03,621 Stage-1 map = 0%,  reduce = 0%
    2017-02-08 21:46:14,538 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.54 sec
    2017-02-08 21:46:24,710 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.11 sec
    MapReduce Total cumulative CPU time: 5 seconds 110 msec
    Ended Job = job_1486604270841_0001
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.11 sec   HDFS Read: 1987008 HDFS Write: 106 SUCCESS
    Total MapReduce CPU Time Spent: 5 seconds 110 msec
    OK
    100000
    Time taken: 47.154 seconds, Fetched: 1 row(s)    


Another operation from the example is as follows::

    hive> CREATE TABLE u_data_new (
        >   userid INT,
        >   movieid INT,
        >   rating INT,
        >   weekday INT)
        > ROW FORMAT DELIMITED
        > FIELDS TERMINATED BY '\t';
    OK
    Time taken: 6.581 seconds
    hive>
        > add FILE weekday_mapper.py;
    Added resources: [weekday_mapper.py]
    hive>
        > INSERT OVERWRITE TABLE u_data_new
        > SELECT
        >   TRANSFORM (userid, movieid, rating, unixtime)
        >   USING 'python weekday_mapper.py'
        >   AS (userid, movieid, rating, weekday)
        > FROM u_data;
    WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
    Query ID = hadoop_20170209055039_2b0302c0-8879-4466-854d-2b68ffcf1e9d
    Total jobs = 3
    Launching Job 1 out of 3
    Number of reduce tasks is set to 0 since there's no reduce operator
    Starting Job = job_1486604270841_0002, Tracking URL = http://localhost:8088/proxy/application_1486604270841_0002/
    Kill Command = /home/hadoop/hadoop/bin/hadoop job  -kill job_1486604270841_0002
    Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
    2017-02-09 05:51:16,902 Stage-1 map = 0%,  reduce = 0%
    2017-02-09 05:51:32,543 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.86 sec
    MapReduce Total cumulative CPU time: 4 seconds 860 msec
    Ended Job = job_1486604270841_0002
    Stage-4 is selected by condition resolver.
    Stage-3 is filtered out by condition resolver.
    Stage-5 is filtered out by condition resolver.
    Moving data to directory hdfs://localhost:9000/user/hive/warehouse/u_data_new/.hive-staging_hive_2017-02-09_05-50-39_371_3480982628034436200-1/-ext-10000
    Loading data to table default.u_data_new
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 1   Cumulative CPU: 4.86 sec   HDFS Read: 1984289 HDFS Write: 1179256 SUCCESS
    Total MapReduce CPU Time Spent: 4 seconds 860 msec
    OK
    Time taken: 56.406 seconds
    hive>
        > SELECT weekday, COUNT(*)
        > FROM u_data_new
        > GROUP BY weekday;
    WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
    Query ID = hadoop_20170209055154_08bfca1c-01e5-4f7a-b6c5-8eed26f1331a
    Total jobs = 1
    Launching Job 1 out of 1
    Number of reduce tasks not specified. Estimated from input data size: 1
    In order to change the average load for a reducer (in bytes):
      set hive.exec.reducers.bytes.per.reducer=<number>
    In order to limit the maximum number of reducers:
      set hive.exec.reducers.max=<number>
    In order to set a constant number of reducers:
      set mapreduce.job.reduces=<number>
    Starting Job = job_1486604270841_0003, Tracking URL = http://localhost:8088/proxy/application_1486604270841_0003/
    Kill Command = /home/hadoop/hadoop/bin/hadoop job  -kill job_1486604270841_0003
    Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
    2017-02-09 05:52:11,506 Stage-1 map = 0%,  reduce = 0%
    2017-02-09 05:52:23,354 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.26 sec
    2017-02-09 05:52:34,530 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.68 sec
    MapReduce Total cumulative CPU time: 4 seconds 680 msec
    Ended Job = job_1486604270841_0003
    MapReduce Jobs Launched:
    Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.68 sec   HDFS Read: 1187554 HDFS Write: 227 SUCCESS
    Total MapReduce CPU Time Spent: 4 seconds 680 msec
    OK
    1	13278
    2	14816
    3	15426
    4	13774
    5	17964
    6	12318
    7	12424
    Time taken: 45.767 seconds, Fetched: 7 row(s)
    

Useful HIVE commands.
==========================

Here are some useful commands.  Some information taken from book **Apache Hive Essentials**  By: Dayong Du

Data Definition Language (DDL )( DDL statements are used to build and modify the tables and other objects in the database.)::
    
    CREATE, DROP, TRUNCATE, ALTER, SHOW, DESCRIBE Statements.   
    USE database; #Select a database
    SHOW DATABASES;
    SHOW TABLES;  #List tables
    DESCRIBE (FORMATTED|EXTENDED) table; #Describing format of table
    CREATE DATABASE db_name; #Creating a database
    DROP DATABASE db_name (CASCADE); #Dropping a database

    
    
    
Data Manipulation Language (DML )(DML statements are used to retrieve, store, modify, delete, insert and update data in the database.)::

    LOAD, INSERT Statements.
    
    LOAD data <LOCAL> inpath <file path> into table [tablename] 
    ADD { FILE[S] | JAR[S] | ARCHIVE[S] } <filepath1> [<filepath2>]*
    LIST { FILE[S] | JAR[S] | ARCHIVE[S] } [<filepath1> <filepath2> ..]
    DELETE { FILE[S] | JAR[S] | ARCHIVE[S] } [<filepath1> <filepath2> ..]
    EXPORT
    IMPORT
    ORDER
    SORT
        
   

.. figure:: _static/sqlcompat2.png

    List of sql commands for HIVE
    
    
Useful HDFS commands
=====================

Here is a list of usefull commands taken from website:  https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html::

    bin/hadoop fs <args>
    hadoop fs -appendToFile <localsrc> ... <dst>
    hadoop fs -cat [-ignoreCrc] URI [URI ...]
    hadoop fs -copyFromLocal <localsrc> URI
    hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>
    hadoop fs -find <path> ... <expression> ...
    hadoop fs -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] <args>
    hadoop fs -mkdir [-p] <paths>
    hadoop fs -tail [-f] URI